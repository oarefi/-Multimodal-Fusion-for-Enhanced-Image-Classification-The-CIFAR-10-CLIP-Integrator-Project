{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt_ISDm2aolo",
        "outputId": "aac7ac47-decc-46ee-bf86-a8bb27097ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fju3zfz9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-fju3zfz9\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=1c8f876c116dfe77648ba23008f4d634c766c834c1214f1f5c8717d29eea2185\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8p4as3wy/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBrrA1YbapJQ",
        "outputId": "2ae240b9-3df3-4bef-ae55-2a1d6b822271"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "# Check if CUDA is available and set device accordingly\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the CLIP model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Define the transformation for CIFAR-10 images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit CLIP's input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Text descriptions for CIFAR-10 classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "text_descriptions = [f\"a photo of a {cls}\" for cls in classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "# Inference loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Calculate image and text features\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "\n",
        "        # Determine similarity and predict class\n",
        "        similarities = (image_features @ text_features.T).softmax(dim=-1)\n",
        "        predicted_classes = similarities.argmax(dim=-1)\n",
        "\n",
        "        # Evaluate predictions\n",
        "        correct += (predicted_classes == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of CLIP on the CIFAR-10 test images: {accuracy}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UC7B483aquj",
        "outputId": "97a35fad-f4bd-4eea-a856-e7e2ec085b6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 109MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29852227.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Accuracy of CLIP on the CIFAR-10 test images: 78.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "# Custom Classifier Definition\n",
        "class CustomCLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model, num_classes):\n",
        "        super(CustomCLIPClassifier, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),  # Adjust dimensions to concatenate image and text features\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text, labels):\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(image).float()  # Convert to float32\n",
        "            text_features = self.clip_model.encode_text(text)[labels].float()  # Convert to float32 and select relevant text features\n",
        "        features = torch.cat((image_features, text_features), dim=1)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# Initialize device, model, and text descriptions\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Text descriptions for CIFAR-10 classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "text_descriptions = [f\"a photo of a {cls}\" for cls in classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "# Instantiate the custom classifier model\n",
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "model = CustomCLIPClassifier(clip_model, num_classes).to(device)\n",
        "\n",
        "# Define the transformation for CIFAR-10 images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit CLIP's input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Set CLIP model parameters to not require gradients\n",
        "for param in model.clip_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5  # Set the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, text_tokens, labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images, text_tokens, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the Custom CLIP Classifier on the CIFAR-10 test images: {accuracy}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4HLG3HFb2BO",
        "outputId": "79760583-6551-4b83-8042-0ca6e591f9d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,   100] loss: 0.060\n",
            "[1,   200] loss: 0.002\n",
            "[1,   300] loss: 0.001\n",
            "[1,   400] loss: 0.001\n",
            "[1,   500] loss: 0.000\n",
            "[1,   600] loss: 0.000\n",
            "[1,   700] loss: 0.000\n",
            "[2,   100] loss: 0.000\n",
            "[2,   200] loss: 0.000\n",
            "[2,   300] loss: 0.000\n",
            "[2,   400] loss: 0.000\n",
            "[2,   500] loss: 0.000\n",
            "[2,   600] loss: 0.000\n",
            "[2,   700] loss: 0.000\n",
            "[3,   100] loss: 0.000\n",
            "[3,   200] loss: 0.000\n",
            "[3,   300] loss: 0.000\n",
            "[3,   400] loss: 0.000\n",
            "[3,   500] loss: 0.000\n",
            "[3,   600] loss: 0.000\n",
            "[3,   700] loss: 0.000\n",
            "[4,   100] loss: 0.000\n",
            "[4,   200] loss: 0.000\n",
            "[4,   300] loss: 0.000\n",
            "[4,   400] loss: 0.000\n",
            "[4,   500] loss: 0.000\n",
            "[4,   600] loss: 0.000\n",
            "[4,   700] loss: 0.000\n",
            "[5,   100] loss: 0.000\n",
            "[5,   200] loss: 0.000\n",
            "[5,   300] loss: 0.000\n",
            "[5,   400] loss: 0.000\n",
            "[5,   500] loss: 0.000\n",
            "[5,   600] loss: 0.000\n",
            "[5,   700] loss: 0.000\n",
            "Finished Training\n",
            "Accuracy of the Custom CLIP Classifier on the CIFAR-10 test images: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import clip\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Custom Classifier Definition\n",
        "class CustomCLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model, num_classes):\n",
        "        super(CustomCLIPClassifier, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),  # Adjust dimensions to concatenate image and text features\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text, labels):\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(image).float()  # Convert to float32\n",
        "            text_features = self.clip_model.encode_text(text)[labels].float()  # Convert to float32 and select relevant text features\n",
        "        features = torch.cat((image_features, text_features), dim=1)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# Initialize device, model, and text descriptions\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "\n",
        "# Text descriptions for CIFAR-10 classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "text_descriptions = [f\"a photo of a {cls}\" for cls in classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "# Transformation for CIFAR-10 images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit CLIP's input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the entire CIFAR-10 dataset (without splitting into train and test)\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# K-Fold Cross-Validation setup\n",
        "k_folds = 5\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "\n",
        "    # Define data loaders for training and validation data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=train_subsampler)\n",
        "    valloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=val_subsampler)\n",
        "\n",
        "    # Init the neural network\n",
        "    model = CustomCLIPClassifier(clip_model, num_classes).to(device)\n",
        "\n",
        "    # Initialize optimizer, loss function, etc.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "    # Set CLIP model parameters to not require gradients\n",
        "    for param in model.clip_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Train the model for this fold\n",
        "    num_epochs = 5  # Set the number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, text_tokens, labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # Print every 100 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    # Evaluate the model on validation set for this fold\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images, text_tokens, labels)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print accuracy for this fold\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy for fold {fold}: {accuracy}% \\n')\n",
        "\n",
        "    # [Record performance metrics for each fold if necessary]\n"
      ],
      "metadata": {
        "id": "glmoPsP0yrJr",
        "outputId": "1f6f6887-92e9-4a2b-c226-d04d14e82327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "FOLD 0\n",
            "--------------------------------\n",
            "[1,   100] loss: 0.059\n",
            "[1,   200] loss: 0.002\n",
            "[1,   300] loss: 0.001\n",
            "[1,   400] loss: 0.001\n",
            "[1,   500] loss: 0.000\n",
            "[1,   600] loss: 0.000\n",
            "[2,   100] loss: 0.000\n",
            "[2,   200] loss: 0.000\n",
            "[2,   300] loss: 0.000\n",
            "[2,   400] loss: 0.000\n",
            "[2,   500] loss: 0.000\n",
            "[2,   600] loss: 0.000\n",
            "[3,   100] loss: 0.000\n",
            "[3,   200] loss: 0.000\n",
            "[3,   300] loss: 0.000\n",
            "[3,   400] loss: 0.000\n",
            "[3,   500] loss: 0.000\n",
            "[3,   600] loss: 0.000\n",
            "[4,   100] loss: 0.000\n",
            "[4,   200] loss: 0.000\n",
            "[4,   300] loss: 0.000\n",
            "[4,   400] loss: 0.000\n",
            "[4,   500] loss: 0.000\n",
            "[4,   600] loss: 0.000\n",
            "[5,   100] loss: 0.000\n",
            "[5,   200] loss: 0.000\n",
            "[5,   300] loss: 0.000\n",
            "[5,   400] loss: 0.000\n",
            "[5,   500] loss: 0.000\n",
            "[5,   600] loss: 0.000\n",
            "Accuracy for fold 0: 100.0% \n",
            "\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "[1,   100] loss: 0.060\n",
            "[1,   200] loss: 0.002\n",
            "[1,   300] loss: 0.001\n",
            "[1,   400] loss: 0.001\n",
            "[1,   500] loss: 0.000\n",
            "[1,   600] loss: 0.000\n",
            "[2,   100] loss: 0.000\n",
            "[2,   200] loss: 0.000\n",
            "[2,   300] loss: 0.000\n",
            "[2,   400] loss: 0.000\n",
            "[2,   500] loss: 0.000\n",
            "[2,   600] loss: 0.000\n",
            "[3,   100] loss: 0.000\n",
            "[3,   200] loss: 0.000\n",
            "[3,   300] loss: 0.000\n",
            "[3,   400] loss: 0.000\n",
            "[3,   500] loss: 0.000\n",
            "[3,   600] loss: 0.000\n",
            "[4,   100] loss: 0.000\n",
            "[4,   200] loss: 0.000\n",
            "[4,   300] loss: 0.000\n",
            "[4,   400] loss: 0.000\n",
            "[4,   500] loss: 0.000\n",
            "[4,   600] loss: 0.000\n",
            "[5,   100] loss: 0.000\n",
            "[5,   200] loss: 0.000\n",
            "[5,   300] loss: 0.000\n",
            "[5,   400] loss: 0.000\n",
            "[5,   500] loss: 0.000\n",
            "[5,   600] loss: 0.000\n",
            "Accuracy for fold 1: 100.0% \n",
            "\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "[1,   100] loss: 0.062\n",
            "[1,   200] loss: 0.002\n",
            "[1,   300] loss: 0.001\n",
            "[1,   400] loss: 0.001\n",
            "[1,   500] loss: 0.000\n",
            "[1,   600] loss: 0.000\n",
            "[2,   100] loss: 0.000\n",
            "[2,   200] loss: 0.000\n",
            "[2,   300] loss: 0.000\n",
            "[2,   400] loss: 0.000\n",
            "[2,   500] loss: 0.000\n",
            "[2,   600] loss: 0.000\n",
            "[3,   100] loss: 0.000\n",
            "[3,   200] loss: 0.000\n",
            "[3,   300] loss: 0.000\n",
            "[3,   400] loss: 0.000\n",
            "[3,   500] loss: 0.000\n",
            "[3,   600] loss: 0.000\n",
            "[4,   100] loss: 0.000\n",
            "[4,   200] loss: 0.000\n",
            "[4,   300] loss: 0.000\n",
            "[4,   400] loss: 0.000\n",
            "[4,   500] loss: 0.000\n",
            "[4,   600] loss: 0.000\n",
            "[5,   100] loss: 0.000\n",
            "[5,   200] loss: 0.000\n",
            "[5,   300] loss: 0.000\n",
            "[5,   400] loss: 0.000\n",
            "[5,   500] loss: 0.000\n",
            "[5,   600] loss: 0.000\n",
            "Accuracy for fold 2: 100.0% \n",
            "\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "[1,   100] loss: 0.054\n",
            "[1,   200] loss: 0.002\n",
            "[1,   300] loss: 0.001\n",
            "[1,   400] loss: 0.000\n",
            "[1,   500] loss: 0.000\n",
            "[1,   600] loss: 0.000\n",
            "[2,   100] loss: 0.000\n",
            "[2,   200] loss: 0.000\n",
            "[2,   300] loss: 0.000\n",
            "[2,   400] loss: 0.000\n",
            "[2,   500] loss: 0.000\n",
            "[2,   600] loss: 0.000\n",
            "[3,   100] loss: 0.000\n",
            "[3,   200] loss: 0.000\n",
            "[3,   300] loss: 0.000\n",
            "[3,   400] loss: 0.000\n",
            "[3,   500] loss: 0.000\n",
            "[3,   600] loss: 0.000\n",
            "[4,   100] loss: 0.000\n",
            "[4,   200] loss: 0.000\n",
            "[4,   300] loss: 0.000\n",
            "[4,   400] loss: 0.000\n",
            "[4,   500] loss: 0.000\n",
            "[4,   600] loss: 0.000\n",
            "[5,   100] loss: 0.000\n",
            "[5,   200] loss: 0.000\n",
            "[5,   300] loss: 0.000\n",
            "[5,   400] loss: 0.000\n",
            "[5,   500] loss: 0.000\n",
            "[5,   600] loss: 0.000\n",
            "Accuracy for fold 3: 100.0% \n",
            "\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "[1,   100] loss: 0.055\n",
            "[1,   200] loss: 0.002\n",
            "[1,   300] loss: 0.001\n",
            "[1,   400] loss: 0.001\n",
            "[1,   500] loss: 0.000\n",
            "[1,   600] loss: 0.000\n",
            "[2,   100] loss: 0.000\n",
            "[2,   200] loss: 0.000\n",
            "[2,   300] loss: 0.000\n",
            "[2,   400] loss: 0.000\n",
            "[2,   500] loss: 0.000\n",
            "[2,   600] loss: 0.000\n",
            "[3,   100] loss: 0.000\n",
            "[3,   200] loss: 0.000\n",
            "[3,   300] loss: 0.000\n",
            "[3,   400] loss: 0.000\n",
            "[3,   500] loss: 0.000\n",
            "[3,   600] loss: 0.000\n",
            "[4,   100] loss: 0.000\n",
            "[4,   200] loss: 0.000\n",
            "[4,   300] loss: 0.000\n",
            "[4,   400] loss: 0.000\n",
            "[4,   500] loss: 0.000\n",
            "[4,   600] loss: 0.000\n",
            "[5,   100] loss: 0.000\n",
            "[5,   200] loss: 0.000\n",
            "[5,   300] loss: 0.000\n",
            "[5,   400] loss: 0.000\n",
            "[5,   500] loss: 0.000\n",
            "[5,   600] loss: 0.000\n",
            "Accuracy for fold 4: 100.0% \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-NZ9nECjA7cO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}